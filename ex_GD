import random
import math
from csv import reader
import re

def load_data(data):  
        mydata=[]
        with open(data, 'r') as a:
            file = reader(a)
            for row in file:
                mydata.append(row)
        return mydata

def set_network(n_input, n_hidden, n_output):
        mynetwork = []
        w_hidden = [{'weights':[random.random() for i in range(n_input+1)]} for i in range(n_hidden)]
        mynetwork.append(w_hidden)
        w_output = [{'weights':[random.random() for i in range(n_hidden+1)]} for i in range(n_output)]
        mynetwork.append(w_output)
        return mynetwork

def calculate_input(weights, inputs):   #### all weights * all inputs + bias(input = 1)
        z = 0.0
        for i in range(len(weights)):
                if i != len(weights)-1:
                        z += weights[i]*inputs[i]
                else:
                        z += weights[i]
        return z

def tanh(z):   ####hyperbolic tangent function
        activation = 2/(1 + math.exp(-2*z)) - 1
        return activation

def forward_propagate(network, inputs):
        input_data = inputs
        for layer in network:   ####layers are lists
                new_inputs = []
                for neuron in layer:  #### dictionary
                        z = calculate_input(neuron['weights'], input_data)
                        neuron['outputs'] = tanh(z)
                        new_inputs.append(neuron['outputs'])    ####this layer's outputs are next layer's inputs 
                input_data = new_inputs
        return input_data   ####last layer's outputs
                        
def tanh_derivative(output):    
        derivative = 1-output**2
        return derivative

def error(network, expect):
        for layer_number in reversed(range(len(network))):
                layer = network[layer_number]
                error = []
                if layer_number == len(network)-1:
                        for neuron_number in range(len(layer)):
                                neuron = layer[neuron_number]
                                error.append((expect[neuron_number] - neuron['outputs']))
                                #### E = (1/2)*(target - output), here we get partial derivative of E with output
                else:
                        for neuron_number in range(len(layer)):
                                previous_layer_error = 0.0
                                for neuron in network[layer_number + 1]:
                                        previous_layer_error += (neuron['weights'][neuron_number] * neuron['delta'])
                                error.append(previous_layer_error)
                for neuron_number in range(len(layer)):
                        neuron = layer[neuron_number]
                        neuron['delta'] = error[neuron_number]*tanh_derivative(neuron['outputs'])

def update_weights(network, learning_rate, data_row):
        inputs = data_row[:-1]
        for layer_number in range(len(network)):
                if layer_number != 0:
                        inputs = [neuron['outputs'] for neuron in network[layer_number-1]]
                for neuron in network[layer_number]:
                        for weights_number in range(len(neuron['weights'])):
                                if weights_number != len(neuron['weights'])-1:
                                        neuron['weights'][weights_number] += learning_rate * neuron['delta'] * inputs[weights_number]
                        neuron['weights'][-1] += learning_rate * neuron['delta']
 
def train(network, mydata, epoch, learning_rate, n_outputs):
        for era in range(epoch):
                for data_row in mydata:
                        non_answer_data_row = data_row[:-1]
                        output = forward_propagate(network, non_answer_data_row)
                        expect = [0 for i in range(n_outputs)]
                        expect[data_row[-1]] = 1
                        error(network, expect)
                        update_weights(network, learning_rate, data_row)
#int main():                        
mydata = load_data('seeds_206_3.csv')

for c in range(len(mydata)):
                mydata[c] = [float(element) for element in re.split('\t+', mydata[c][0])]
                mydata[c][-1] = int(mydata[c][-1])-1
                #### original, the answer is 1, 2, 3,but we need 0, 1, 2(because lists),than use one-hot 
n_inputs = len(mydata[0])-1
n_outputs = 3
learning_rate = 0.3
epoch = 5000
n_hidden = 5
network = set_network(n_inputs, n_hidden, n_outputs)

for layer in network:
	print(layer)
	
train(network, mydata, 500, 0.3, n_outputs)

for layer in network:
	print(layer)

#network = [[{'output': 0.7105668883115941, 'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],
		#[{'output': 0.6213859615555266, 'weights': [0.2550690257394217, 0.49543508709194095]}, {'output': 0.6573693455986976, 'weights': [0.4494910647887381, 0.651592972722763]}]]
